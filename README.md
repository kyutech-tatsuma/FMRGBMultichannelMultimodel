# FMRGBマルチチャネルマルチモデル
音の周波数と振幅、画像のRGB情報をチャンネルとした動画データを作成し、学習を行うモデルです。
ネットワークには、3次元の畳み込みニューラルネットワークを採用しています。

## 利用方法
1. データセットの作成
```
python create_fmrgbdata.py --target <mp4データが格納されているディレクトリーパス> --audiomethod <simple or mel>
```
target: fmrgbデータに変換したい動画データが格納されているディレクトリの指定

audiometho: 音声データの解析をシンプルなスペクトログラムで解析するかメロスペクトログラムで解析するかをしてするオプション

### 音声データの画像化
スペクトログラム

    1. 最初にpydubを使用してMP4ファイルから音声データを抽出します。

    2. scipy.signal.spectrogramを使用して音声データからスペクトログラムを計算します。スペクトログラムは音声信号の周波数特性を時間ごとに表したものです。

    3. 最後にスペクトログラムを画像に変換します。周波数と振幅の最小値と最大値を0から255の範囲に線形補間し、それを8ビットの整数に変換します。これにより、スペクトログラムがグレースケールの画像として表現されます。

メルスペクトログラム

    1. 最初にpydubを使用してMP4ファイルから音声データを抽出します。

    2. librosaを使用してこのwavファイルを読み込み、音声データからメルスペクトログラムを計算します。メルスペクトログラムは音声信号の周波数の特徴を時間ごとに表したものです。

    3. メルスペクトログラムを対数スケールに変換します。対数スケールに変換することで音声データのダイナミックレンジが人間の近くにより近くなります。

    4. 最後に、対数スケールのメルスペクトログラムを画像に変換します。ここでは、メルスペクトログラムの最小値と最大値を0から255の範囲に線形補間し、それを整数に変換します。これにより、メルスペクトログラムがグレースケールの画像として表現されます。

### データの長さの揃える方法
    1. ディレクトリ内の全ての動画を調査し、フレーム数が最も多い動画のフレーム数を取得します。

    2. その後、各動画を処理する際に動画のフレーム数が最大フレーム数より少ない場合に、フレーム数を最大フレーム数にリサイズします。リサイズにはscipy.ndimage.zoom関数を用います。この関数は、入力配列の各次元を特定のズーム因子に基づいて拡大または縮小します。ズーム因子は、現在のフレーム数を最大フレームで割ったもので計算されます。
2. データの可視化
```
python visualize.py --target <ptファイル> --result <結果を格納するディレクトリパス>
```
3. FMRGBマルチチャネルモデルの学習
```
python train.py --data <csvデータ> --epochs <学習回数> --test_size <テストデータの割合指定> --patience <早期終了パラメータ>
```
